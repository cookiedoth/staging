---
layout: distill
title: Graph neural networks v.s. transformers for geometric graphs
description: With the recent development of graph transformers, in this project we aim to compare their performance on a molecular task of protein-ligand binding affinity prediction against the performance of message passing graph neural networks.
date: 2023-11-01
htmlwidgets: true

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Ada Fang
    affiliations:
      name: MIT

# must be the exact same name as your blogpost
bibliography: 2023-11-09-proposal.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
toc:
  - name: Introduction
  - name: Relevant work
    subsections:
    - name: Graph neural networks 
    - name: Graph transformers
  - name: Problem definition
  - name: Dataset
  - name: Proposed experiments
    subsections:
    - name: Proposed algorithmic contributions
    - name: Can transformers better capture long range interactions
    - name: Can graph neural networks approximate transformers with a fully connected graph

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Introduction
Machine learning on graphs is often approached with message passing graph neural network (GNN) models where nodes in the graph are embedded with aggregated messages passed from neighboring nodes <d-cite key=zhou2020graph></d-cite>. However, with the significant success of transformers in language modelling <d-cite key=vaswani2017attention></d-cite> and computer vision recently <d-cite key=dosovitskiy2020image></d-cite>, there are a growing number of transformers developed for graphs as well. In this project we investigate the application of graph neural networks compared to transformers on geometric graphs defined on point clouds. We aim to explore the performance of these two models on predicting the binding affinity for a protein-ligand interaction given the atomic coordinates of the docked protein-ligand structure, which is a highly relevant task in drug discovery.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2023-11-09-proposal/protein-ligand-structure.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    A protein-ligand structure, Protein Data Bank (PDB) entry 1a0q. The protein backbone is shown in blue, and the ligand is shown in green. The model would be given this structure and the objective is to predict the binding affinity of the ligand to the protein. 
</div>

## Relevant work
Early applications of machine learning on molecules were mainly with graph neural networks. However, with the proliferation of transformers in the machine learning field, this has also influenced the development of graph transformers. Here we summarise a few key contributions in these two model archetypes for molecules. 

### Graph neural networks
Here we focus on some key works on SE(3)-equivariant graph neural networks--where model outputs transform in the same way as inputs under 3D global translations and rotations--which are effective for modelling geometric data. Early graph neural networks on point clouds which used directional message passing <d-cite key="gasteiger2020directional"></d-cite> were limited in expressivity <d-cite key="garg2020generalization"></d-cite>. Now state-of-the-art (SOTA) models in this area are based on higher order geometric properties such as dihedral angles and representations in the geometric group SO(3). Some examples include GemNet  <d-cite key=gasteiger2021gemnet></d-cite> and e3nn <d-cite key=geiger2022e3nn></d-cite>. These models have led to exceptional performance for tasks related to predicting molecular forces and energies <d-cite key=batzner20223></d-cite> <d-cite key=musaelian2023learning></d-cite>. For the task of binding affinity some models that achieve high performance using GNNs are from the following papers <d-cite key=wang2022learning></d-cite> <d-cite key=somnath2021multi></d-cite>.

### Graph transformers
Graph transformers have also been applied to molecules for property prediction. Graph transformers and sequence transformers are largely similar in architecture; however, differences arise in the positional encodings in a graph transformer as it is defined in relation to other nodes in the graph <d-cite key=ying2021transformers></d-cite>. For geometric graphs, positional encodings can be applied as a bias term on the attention value of node $u$ on $v$, where the bias is a learned value that is dependent on the distance between the nodes <d-cite key=zhou2023uni></d-cite> <d-cite key=luo2022one></d-cite>. There are also other ways of implementing positional encodings in the form of Laplacian eigenvectors, and random walk diagonals <d-cite key=rampavsek2022recipe></d-cite>. Recently, in an effort to unify different methods to generate structural and positional graph encodings, Liu et al. <d-cite key=liu2023graph></d-cite> apply a novel pretraining approach with a multiobjective task of learning a variety of positional and structural encodings to derive more general positional and structural encodings. Graph transformers are also achieving SOTA performance for benchmarks on predicting quantum properties of molecules <d-cite key=zhou2023uni></d-cite> <d-cite key=luo2022one></d-cite> and binding affinity <d-cite key=kong2023generalist></d-cite>.

## Problem definition
The input to the model is a set of atoms for the protein $X_{\mathrm{protein}}$ and ligand $X_{\mathrm{ligand}}$, for which we have the atomic identity and the 3D coordinates, and the binding affinity $y$ for the structure. For the graph neural network we define a molecular graph of the protein ligand structure $G=(V,E)$ where $V$ are the $n$ nodes that represent atoms in the molecule and the edges $E$ are defined between two nodes if their 3D distance is within a radial cutoff $r$. For the graph transformer it is applied to the whole set of atoms $(X_{\mathrm{protein}}, X_{\mathrm{ligand}})$, and we can use the 3D coordinates of the atoms to derive positional encodings. Performance is determined by the root mean squared error, Pearson, and Spearman correlation coefficients between true binding affinity and predicted binding affinity. 

## Dataset
We use the PDBbind dataset for the protein-ligand structures and binding affinity. In addition, for benchmarking we use the benchmark from ATOM3D <d-cite key="townshend2020atom3d"></d-cite> with a 30% and 60% sequence identity split on the protein to better test generalisability of the model.


## Proposed experiments
We will implement two models, a SE(3)-equivariant graph neural network based on Tensor Field Networks using e3nn <d-cite key=geiger2022e3nn></d-cite> and DiffDock <d-cite key=corso2022diffdock></d-cite> (a protein-ligand docking model), and a graph transformer based on the architecture proposed by Transformer-M <d-cite key=luo2022one></d-cite>. For fair comparison we will ensure the number of trainable parameters in both models is comparable by adjusting the number of layers and embedding dimension. The models will be trained to convergence on the ATOM3D dataset split and the best performing model on the validation split will be used to evaluate the test split.

### Proposed algorithmic contributions
For the GNN we will use the confidence model in DiffDock <d-cite key=corso2022diffdock></d-cite> as an analogy to the binding affinity predictor model. The confidence model in DiffDock is given a docked protein-ligand structure and it scores how likely the structure is within 2 $\overset{\circ}{A}$ to the true structure. Similarly, the binding affinity model will be given the coordinates of the experimental protein-ligand structure and will predict the protein-ligand binding affinity.

For the transformer, Transformer-M <d-cite key=luo2022one></d-cite> is pretrained on a broad set of 2D and 3D molecular structures and has been finetuned to predict protein-ligand binding affinity. However, we would like to compare this to a GNN model in a fair way, which would require using the Transformer-M architecture for only the 3D structure input track and predicting binding affinity with only the training dataset.

### Can transformers better capture long range interactions
Fundamentally, transformers vary from graph neural networks with their ability to capture long range interactions compared to the $k$-hop neighbourhoods that can be captured by a $k$-layer graph neural network. We explore how model performance is a function of graph size and diameter for the two model archetypes to see if transformers are better at capturing long range interactions. We will also isolate subsets of molecules where the models achieve the best and worse performance to compare if the models are excelling in similar areas.

### Can graph neural networks approximate transformers with a fully connected graph
One of the fundamental differences between transformers and GNNs is the neighborhood of nodes that each node receives updates from. For a transformer this is all nodes in a graph, and for a GNN this is the $k$-hop neighborhood. To bridge these differences we can construct a fully connected graph by increasing the radial cutoff $r$ for edges in the graph. We want to test for a GNN trained on a fully connected graph if we would achieve similar performance to the graph transformer. 