
@misc{zhong_clock_2023,
	title = {The {Clock} and the {Pizza}: {Two} {Stories} in {Mechanistic} {Explanation} of {Neural} {Networks}},
	shorttitle = {The {Clock} and the {Pizza}},
	url = {http://arxiv.org/abs/2306.17844},
	abstract = {Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms for solving those tasks? Several recent studies, on tasks ranging from group arithmetic to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex. Small changes to model hyperparameters and initializations can induce the discovery of qualitatively different algorithms from a fixed training set, and even parallel implementations of multiple such algorithms. Some networks trained to perform modular addition implement a familiar Clock algorithm; others implement a previously undescribed, less intuitive, but comprehensible procedure which we term the Pizza algorithm, or a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools for characterizing the behavior of neural networks across their algorithmic phase space.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
	month = jun,
	year = {2023},
	note = {arXiv:2306.17844 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/vedanglad/Zotero/storage/BV3SZYTS/Zhong et al. - 2023 - The Clock and the Pizza Two Stories in Mechanisti.pdf:application/pdf;arXiv.org Snapshot:/Users/vedanglad/Zotero/storage/YULYZZCZ/2306.html:text/html},
}

@misc{nanda_progress_2023,
	title = {Progress measures for grokking via mechanistic interpretability},
	url = {http://arxiv.org/abs/2301.05217},
	abstract = {Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous {\textbackslash}textit\{progress measures\} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
	month = oct,
	year = {2023},
	note = {arXiv:2301.05217 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 10 page main body, 2 page references, 24 page appendix},
	file = {arXiv Fulltext PDF:/Users/vedanglad/Zotero/storage/TMGAKUHX/Nanda et al. - 2023 - Progress measures for grokking via mechanistic int.pdf:application/pdf;arXiv.org Snapshot:/Users/vedanglad/Zotero/storage/22CHS4X6/2301.html:text/html},
}

@misc{rauker_toward_2023,
	title = {Toward {Transparent} {AI}: {A} {Survey} on {Interpreting} the {Inner} {Structures} of {Deep} {Neural} {Networks}},
	shorttitle = {Toward {Transparent} {AI}},
	url = {http://arxiv.org/abs/2207.13243},
	abstract = {The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.},
	urldate = {2023-11-09},
	publisher = {arXiv},
	author = {Räuker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
	month = aug,
	year = {2023},
	note = {arXiv:2207.13243 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/vedanglad/Zotero/storage/IW5Q98HE/Räuker et al. - 2023 - Toward Transparent AI A Survey on Interpreting th.pdf:application/pdf;arXiv.org Snapshot:/Users/vedanglad/Zotero/storage/9T4XIKQK/2207.html:text/html},
}
