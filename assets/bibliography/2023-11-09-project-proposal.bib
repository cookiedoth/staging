@misc{chenScalingVisionTransformers2022,
  title = {Scaling {{Vision Transformers}} to {{Gigapixel Images}} via {{Hierarchical Self-Supervised Learning}}},
  author = {Chen, Richard J. and Chen, Chengkuan and Li, Yicong and Chen, Tiffany Y. and Trister, Andrew D. and Krishnan, Rahul G. and Mahmood, Faisal},
  year = {2022},
  month = jun,
  number = {arXiv:2206.02647},
  eprint = {2206.02647},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.02647},
  urldate = {2023-11-09},
  abstract = {Vision Transformers (ViTs) and their multi-scale and hierarchical variations have been successful at capturing image representations but their use has been generally studied for low-resolution images (e.g. - 256x256, 384384). For gigapixel whole-slide imaging (WSI) in computational pathology, WSIs can be as large as 150000x150000 pixels at 20X magnification and exhibit a hierarchical structure of visual tokens across varying resolutions: from 16x16 images capture spatial patterns among cells, to 4096x4096 images characterizing interactions within the tissue microenvironment. We introduce a new ViT architecture called the Hierarchical Image Pyramid Transformer (HIPT), which leverages the natural hierarchical structure inherent in WSIs using two levels of self-supervised learning to learn high-resolution image representations. HIPT is pretrained across 33 cancer types using 10,678 gigapixel WSIs, 408,218 4096x4096 images, and 104M 256x256 images. We benchmark HIPT representations on 9 slide-level tasks, and demonstrate that: 1) HIPT with hierarchical pretraining outperforms current state-of-the-art methods for cancer subtyping and survival prediction, 2) self-supervised ViTs are able to model important inductive biases about the hierarchical structure of phenotypes in the tumor microenvironment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/fm813/Zotero/storage/FRTZ5ZCG/Chen et al. - 2022 - Scaling Vision Transformers to Gigapixel Images vi.pdf;/Users/fm813/Zotero/storage/XJ4CYMYA/2206.html}
}

@article{hackingDeepLearningClassification2021,
  title = {Deep Learning for the Classification of Medical Kidney Disease: A Pilot Study for Electron Microscopy},
  shorttitle = {Deep Learning for the Classification of Medical Kidney Disease},
  author = {Hacking, Sean and Bijol, Vanesa},
  year = {2021},
  month = feb,
  journal = {Ultrastructural Pathology},
  volume = {45},
  pages = {1--10},
  doi = {10.1080/01913123.2021.1882628},
  abstract = {Artificial intelligence (AI) is a new frontier and often enigmatic for medical professionals. Cloud computing could open up the field of computer vision to a wider medical audience and deep learning on the cloud allows one to design, develop, train and deploy applications with ease. In the field of histopathology, the implementation of various applications in AI has been successful for whole slide images rich in biological diversity. However, the analysis of other tissue medias, including electron microscopy, is yet to be explored. The present study aims to evaluate deep learning for the classification of medical kidney disease on electron microscopy images: amyloidosis, diabetic glomerulosclerosis, membranous nephropathy, membranoproliferative glomerulonephritis (MPGN), and thin basement membrane disease (TBMD). We found good overall classification with the MedKidneyEM-v1 Classifier and when looking at normal and diseased kidneys, the average area under the curve for precision and recall was 0.841. The average area under the curve for precision and recall on the disease only cohort was 0.909. Digital pathology will shape a new era for medical kidney disease and the present study demonstrates the feasibility of deep learning for electron microscopy. Future approaches could be used by renal pathologists to improve diagnostic concordance, determine therapeutic strategies, and optimize patient outcomes in a true clinical environment.},
  file = {/Users/fm813/Zotero/storage/ZGY7BPJI/Hacking and Bijol - 2021 - Deep learning for the classification of medical ki.pdf}
}

@misc{reedScaleMAEScaleAwareMasked2023,
  title = {Scale-{{MAE}}: {{A Scale-Aware Masked Autoencoder}} for {{Multiscale Geospatial Representation Learning}}},
  shorttitle = {Scale-{{MAE}}},
  author = {Reed, Colorado J. and Gupta, Ritwik and Li, Shufan and Brockman, Sarah and Funk, Christopher and Clipp, Brian and Keutzer, Kurt and Candido, Salvatore and Uyttendaele, Matt and Darrell, Trevor},
  year = {2023},
  month = sep,
  number = {arXiv:2212.14532},
  eprint = {2212.14532},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.14532},
  urldate = {2023-11-09},
  abstract = {Large, pretrained models are commonly finetuned with imagery that is heavily augmented to mimic different conditions and scales, with the resulting models used for various tasks with imagery from a range of spatial scales. Such models overlook scale-specific information in the data for scale-dependent domains, such as remote sensing. In this paper, we present Scale-MAE, a pretraining method that explicitly learns relationships between data at different, known scales throughout the pretraining process. Scale-MAE pretrains a network by masking an input image at a known input scale, where the area of the Earth covered by the image determines the scale of the ViT positional encoding, not the image resolution. Scale-MAE encodes the masked image with a standard ViT backbone, and then decodes the masked image through a bandpass filter to reconstruct low/high frequency images at lower/higher scales. We find that tasking the network with reconstructing both low/high frequency images leads to robust multiscale representations for remote sensing imagery. Scale-MAE achieves an average of a \$2.4 - 5.6\textbackslash\%\$ non-parametric kNN classification improvement across eight remote sensing datasets compared to current state-of-the-art and obtains a \$0.9\$ mIoU to \$1.7\$ mIoU improvement on the SpaceNet building segmentation transfer task for a range of evaluation scales.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/fm813/Zotero/storage/STV86EH7/Reed et al. - 2023 - Scale-MAE A Scale-Aware Masked Autoencoder for Mu.pdf;/Users/fm813/Zotero/storage/APYW6DED/2212.html}
}

@article{zhangDeepLearningbasedMultimodel2023,
  title = {Deep Learning-Based Multi-Model Approach on Electron Microscopy Image of Renal Biopsy Classification},
  author = {Zhang, Jingyuan and Zhang, Aihua},
  year = {2023},
  month = may,
  journal = {BMC Nephrology},
  volume = {24},
  number = {1},
  pages = {132},
  issn = {1471-2369},
  doi = {10.1186/s12882-023-03182-6},
  urldate = {2023-11-09},
  abstract = {Electron microscopy is important in the diagnosis of renal disease. For immune-mediated renal disease diagnosis, whether the electron-dense granule is present in the electron microscope image is of vital importance. Deep learning methods perform well at feature extraction and assessment of histologic images. However, few studies on deep learning methods for electron microscopy images of renal biopsy have been published. This study aimed to develop a deep learning-based multi-model to automatically detect whether the electron-dense granule is present in the TEM image of renal biopsy, and then help diagnose immune-mediated renal disease.},
  keywords = {Biopsy,Deep Learning,Diagnostic Imaging,Model,Renal},
  file = {/Users/fm813/Zotero/storage/VMZIHWV4/Zhang and Zhang - 2023 - Deep learning-based multi-model approach on electr.pdf;/Users/fm813/Zotero/storage/ZNJIPNEU/s12882-023-03182-6.html}
}
