@inproceedings{lohrenz2023relaxed,
  title={Relaxed attention for transformer models},
  author={Lohrenz, Timo and M{\"o}ller, Bj{\"o}rn and Li, Zhengyang and Fingscheidt, Tim},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--10},
  year={2023},
  organization={IEEE}
}

@article{zehui2019dropattention,
  title={Dropattention: A regularization method for fully-connected self-attention networks},
  author={Zehui, Lin and Liu, Pengfei and Huang, Luyao and Chen, Junkun and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:1907.11065},
  year={2019}
}

@article{zhang2022dropdim,
  title={Dropdim: A regularization method for transformer networks},
  author={Zhang, Hao and Qu, Dan and Shao, Keji and Yang, Xukui},
  journal={IEEE Signal Processing Letters},
  volume={29},
  pages={474--478},
  year={2022},
  publisher={IEEE}
}

@article{li2018multi,
  title={Multi-head attention with disagreement regularization},
  author={Li, Jian and Tu, Zhaopeng and Yang, Baosong and Lyu, Michael R and Zhang, Tong},
  journal={arXiv preprint arXiv:1810.10183},
  year={2018}
}